{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from keras.preprocessing.image import save_img\n",
    "import os\n",
    "import shutil \n",
    "import requests\n",
    "import pandas as pd \n",
    "import csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directory(dirname):\n",
    "    current_path=os.getcwd()\n",
    "    path=os.path.join(current_path,dirname)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_images(data,dirname,page):\n",
    "    for index,link in enumerate(data['images']):\n",
    "        print(\"downloading {0} of {1} images\".format(index + 1, len(data['images'])))\n",
    "        response=requests.get(link)\n",
    "        image_file = io.BytesIO(response)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        with open('{0}/img_{1} {2}.jpeg'.format(dirname,page,index),\"wb\") as file:\n",
    "            file.write(response.content)\n",
    "            image.save(file, \"JPEG\", quality=85)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path=r\"C:\\Users\\prince\\Downloads\\chromedriver_win32\\chromedriver.exe \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME=\"MEN\"\n",
    "make_directory(DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 84 images\n",
      "downloading 2 of 84 images\n",
      "downloading 3 of 84 images\n",
      "downloading 4 of 84 images\n",
      "downloading 5 of 84 images\n",
      "downloading 6 of 84 images\n",
      "downloading 7 of 84 images\n",
      "downloading 8 of 84 images\n",
      "downloading 9 of 84 images\n",
      "downloading 10 of 84 images\n",
      "downloading 11 of 84 images\n",
      "downloading 12 of 84 images\n",
      "downloading 13 of 84 images\n",
      "downloading 14 of 84 images\n",
      "downloading 15 of 84 images\n",
      "downloading 16 of 84 images\n",
      "downloading 17 of 84 images\n",
      "downloading 18 of 84 images\n",
      "downloading 19 of 84 images\n",
      "downloading 20 of 84 images\n",
      "downloading 21 of 84 images\n",
      "downloading 22 of 84 images\n",
      "downloading 23 of 84 images\n",
      "downloading 24 of 84 images\n",
      "downloading 25 of 84 images\n",
      "downloading 26 of 84 images\n",
      "downloading 27 of 84 images\n",
      "downloading 28 of 84 images\n",
      "downloading 29 of 84 images\n",
      "downloading 30 of 84 images\n",
      "downloading 31 of 84 images\n",
      "downloading 32 of 84 images\n",
      "downloading 33 of 84 images\n",
      "downloading 34 of 84 images\n",
      "downloading 35 of 84 images\n",
      "downloading 36 of 84 images\n",
      "downloading 37 of 84 images\n",
      "downloading 38 of 84 images\n",
      "downloading 39 of 84 images\n",
      "downloading 40 of 84 images\n",
      "downloading 41 of 84 images\n",
      "downloading 42 of 84 images\n",
      "downloading 43 of 84 images\n",
      "downloading 44 of 84 images\n",
      "downloading 45 of 84 images\n",
      "downloading 46 of 84 images\n",
      "downloading 47 of 84 images\n",
      "downloading 48 of 84 images\n",
      "downloading 49 of 84 images\n",
      "downloading 50 of 84 images\n",
      "downloading 51 of 84 images\n",
      "downloading 52 of 84 images\n",
      "downloading 53 of 84 images\n",
      "downloading 54 of 84 images\n",
      "downloading 55 of 84 images\n",
      "downloading 56 of 84 images\n",
      "downloading 57 of 84 images\n",
      "downloading 58 of 84 images\n",
      "downloading 59 of 84 images\n",
      "downloading 60 of 84 images\n",
      "downloading 61 of 84 images\n",
      "downloading 62 of 84 images\n",
      "downloading 63 of 84 images\n",
      "downloading 64 of 84 images\n",
      "downloading 65 of 84 images\n",
      "downloading 66 of 84 images\n",
      "downloading 67 of 84 images\n",
      "downloading 68 of 84 images\n",
      "downloading 69 of 84 images\n",
      "downloading 70 of 84 images\n",
      "downloading 71 of 84 images\n",
      "downloading 72 of 84 images\n",
      "downloading 73 of 84 images\n",
      "downloading 74 of 84 images\n",
      "downloading 75 of 84 images\n",
      "downloading 76 of 84 images\n",
      "downloading 77 of 84 images\n",
      "downloading 78 of 84 images\n",
      "downloading 79 of 84 images\n",
      "downloading 80 of 84 images\n",
      "downloading 81 of 84 images\n",
      "downloading 82 of 84 images\n",
      "downloading 83 of 84 images\n",
      "downloading 84 of 84 images\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path=driver_path)\n",
    "page_shirt_men=driver.get(\"https://www.amazon.in/gp/browse.html?node=1968093031&ref_=nav_em_0_2_10_4_sbc_mfashion_shirts\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='octopus-pc-item-image octopus-pc-item-image-v3']\")\n",
    "    \n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    \n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data    \n",
    "\n",
    "product_details=scrap_images(driver=driver)\n",
    "save_images(data=product_details,dirname=DIR_NAME,page=1)\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 60 images\n",
      "downloading 2 of 60 images\n",
      "downloading 3 of 60 images\n",
      "downloading 4 of 60 images\n",
      "downloading 5 of 60 images\n",
      "downloading 6 of 60 images\n",
      "downloading 7 of 60 images\n",
      "downloading 8 of 60 images\n",
      "downloading 9 of 60 images\n",
      "downloading 10 of 60 images\n",
      "downloading 11 of 60 images\n",
      "downloading 12 of 60 images\n",
      "downloading 13 of 60 images\n",
      "downloading 14 of 60 images\n",
      "downloading 15 of 60 images\n",
      "downloading 16 of 60 images\n",
      "downloading 17 of 60 images\n",
      "downloading 18 of 60 images\n",
      "downloading 19 of 60 images\n",
      "downloading 20 of 60 images\n",
      "downloading 21 of 60 images\n",
      "downloading 22 of 60 images\n",
      "downloading 23 of 60 images\n",
      "downloading 24 of 60 images\n",
      "downloading 25 of 60 images\n",
      "downloading 26 of 60 images\n",
      "downloading 27 of 60 images\n",
      "downloading 28 of 60 images\n",
      "downloading 29 of 60 images\n",
      "downloading 30 of 60 images\n",
      "downloading 31 of 60 images\n",
      "downloading 32 of 60 images\n",
      "downloading 33 of 60 images\n",
      "downloading 34 of 60 images\n",
      "downloading 35 of 60 images\n",
      "downloading 36 of 60 images\n",
      "downloading 37 of 60 images\n",
      "downloading 38 of 60 images\n",
      "downloading 39 of 60 images\n",
      "downloading 40 of 60 images\n",
      "downloading 41 of 60 images\n",
      "downloading 42 of 60 images\n",
      "downloading 43 of 60 images\n",
      "downloading 44 of 60 images\n",
      "downloading 45 of 60 images\n",
      "downloading 46 of 60 images\n",
      "downloading 47 of 60 images\n",
      "downloading 48 of 60 images\n",
      "downloading 49 of 60 images\n",
      "downloading 50 of 60 images\n",
      "downloading 51 of 60 images\n",
      "downloading 52 of 60 images\n",
      "downloading 53 of 60 images\n",
      "downloading 54 of 60 images\n",
      "downloading 55 of 60 images\n",
      "downloading 56 of 60 images\n",
      "downloading 57 of 60 images\n",
      "downloading 58 of 60 images\n",
      "downloading 59 of 60 images\n",
      "downloading 60 of 60 images\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(executable_path=driver_path)\n",
    "page_shirt_men=driver.get(\"https://www.amazon.in/s?rh=n%3A1571271031%2Cn%3A%211571272031%2Cn%3A1968024031%2Cn%3A1968093031&page=2&qid=1593613109&ref=lp_1968093031_pg_2\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "    \n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    \n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data    \n",
    "\n",
    "product_details=scrap_images(driver=driver)\n",
    "save_images(data=product_details,dirname=DIR_NAME,page=2)\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME=\"men2\"\n",
    "make_directory(DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 60 images\n",
      "downloading 2 of 60 images\n",
      "downloading 3 of 60 images\n",
      "downloading 4 of 60 images\n",
      "downloading 5 of 60 images\n",
      "downloading 6 of 60 images\n",
      "downloading 7 of 60 images\n",
      "downloading 8 of 60 images\n",
      "downloading 9 of 60 images\n",
      "downloading 10 of 60 images\n",
      "downloading 11 of 60 images\n",
      "downloading 12 of 60 images\n",
      "downloading 13 of 60 images\n",
      "downloading 14 of 60 images\n",
      "downloading 15 of 60 images\n",
      "downloading 16 of 60 images\n",
      "downloading 17 of 60 images\n",
      "downloading 18 of 60 images\n",
      "downloading 19 of 60 images\n",
      "downloading 20 of 60 images\n",
      "downloading 21 of 60 images\n",
      "downloading 22 of 60 images\n",
      "downloading 23 of 60 images\n",
      "downloading 24 of 60 images\n",
      "downloading 25 of 60 images\n",
      "downloading 26 of 60 images\n",
      "downloading 27 of 60 images\n",
      "downloading 28 of 60 images\n",
      "downloading 29 of 60 images\n",
      "downloading 30 of 60 images\n",
      "downloading 31 of 60 images\n",
      "downloading 32 of 60 images\n",
      "downloading 33 of 60 images\n",
      "downloading 34 of 60 images\n",
      "downloading 35 of 60 images\n",
      "downloading 36 of 60 images\n",
      "downloading 37 of 60 images\n",
      "downloading 38 of 60 images\n",
      "downloading 39 of 60 images\n",
      "downloading 40 of 60 images\n",
      "downloading 41 of 60 images\n",
      "downloading 42 of 60 images\n",
      "downloading 43 of 60 images\n",
      "downloading 44 of 60 images\n",
      "downloading 45 of 60 images\n",
      "downloading 46 of 60 images\n",
      "downloading 47 of 60 images\n",
      "downloading 48 of 60 images\n",
      "downloading 49 of 60 images\n",
      "downloading 50 of 60 images\n",
      "downloading 51 of 60 images\n",
      "downloading 52 of 60 images\n",
      "downloading 53 of 60 images\n",
      "downloading 54 of 60 images\n",
      "downloading 55 of 60 images\n",
      "downloading 56 of 60 images\n",
      "downloading 57 of 60 images\n",
      "downloading 58 of 60 images\n",
      "downloading 59 of 60 images\n",
      "downloading 60 of 60 images\n"
     ]
    }
   ],
   "source": [
    "driver2=webdriver.Chrome(executable_path=driver_path)\n",
    "page_tshirts_men=driver2.get(\"https://www.amazon.in/gp/browse.html?node=1968120031&ref_=nav_em_sbc_mfashion_tshirts_0_2_10_3\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "    \n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    \n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data\n",
    "product_details=scrap_images(driver=driver2)\n",
    "save_images(data=product_details,dirname=DIR_NAME,page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 60 images\n",
      "downloading 2 of 60 images\n",
      "downloading 3 of 60 images\n",
      "downloading 4 of 60 images\n",
      "downloading 5 of 60 images\n",
      "downloading 6 of 60 images\n",
      "downloading 7 of 60 images\n",
      "downloading 8 of 60 images\n",
      "downloading 9 of 60 images\n",
      "downloading 10 of 60 images\n",
      "downloading 11 of 60 images\n",
      "downloading 12 of 60 images\n",
      "downloading 13 of 60 images\n",
      "downloading 14 of 60 images\n",
      "downloading 15 of 60 images\n",
      "downloading 16 of 60 images\n",
      "downloading 17 of 60 images\n",
      "downloading 18 of 60 images\n",
      "downloading 19 of 60 images\n",
      "downloading 20 of 60 images\n",
      "downloading 21 of 60 images\n",
      "downloading 22 of 60 images\n",
      "downloading 23 of 60 images\n",
      "downloading 24 of 60 images\n",
      "downloading 25 of 60 images\n",
      "downloading 26 of 60 images\n",
      "downloading 27 of 60 images\n",
      "downloading 28 of 60 images\n",
      "downloading 29 of 60 images\n",
      "downloading 30 of 60 images\n",
      "downloading 31 of 60 images\n",
      "downloading 32 of 60 images\n",
      "downloading 33 of 60 images\n",
      "downloading 34 of 60 images\n",
      "downloading 35 of 60 images\n",
      "downloading 36 of 60 images\n",
      "downloading 37 of 60 images\n",
      "downloading 38 of 60 images\n",
      "downloading 39 of 60 images\n",
      "downloading 40 of 60 images\n",
      "downloading 41 of 60 images\n",
      "downloading 42 of 60 images\n",
      "downloading 43 of 60 images\n",
      "downloading 44 of 60 images\n",
      "downloading 45 of 60 images\n",
      "downloading 46 of 60 images\n",
      "downloading 47 of 60 images\n",
      "downloading 48 of 60 images\n",
      "downloading 49 of 60 images\n",
      "downloading 50 of 60 images\n",
      "downloading 51 of 60 images\n",
      "downloading 52 of 60 images\n",
      "downloading 53 of 60 images\n",
      "downloading 54 of 60 images\n",
      "downloading 55 of 60 images\n",
      "downloading 56 of 60 images\n",
      "downloading 57 of 60 images\n",
      "downloading 58 of 60 images\n",
      "downloading 59 of 60 images\n",
      "downloading 60 of 60 images\n"
     ]
    }
   ],
   "source": [
    "driver2=webdriver.Chrome(executable_path=driver_path)\n",
    "page_tshirts_men=driver2.get(\"https://www.amazon.in/s?k=Men%27s+T-Shirts+%26+Polos&page=2&_encoding=UTF8&c=ts&qid=1593613921&ts_id=1968120031&ref=sr_pg_2\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "    \n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    \n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data    \n",
    "\n",
    "product_details=scrap_images(driver=driver2)\n",
    "save_images(data=product_details,dirname=DIR_NAME,page=2)\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME=\"WOMEN\"\n",
    "make_directory(DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 51 images\n",
      "downloading 2 of 51 images\n",
      "downloading 3 of 51 images\n",
      "downloading 4 of 51 images\n",
      "downloading 5 of 51 images\n",
      "downloading 6 of 51 images\n",
      "downloading 7 of 51 images\n",
      "downloading 8 of 51 images\n",
      "downloading 9 of 51 images\n",
      "downloading 10 of 51 images\n",
      "downloading 11 of 51 images\n",
      "downloading 12 of 51 images\n",
      "downloading 13 of 51 images\n",
      "downloading 14 of 51 images\n",
      "downloading 15 of 51 images\n",
      "downloading 16 of 51 images\n",
      "downloading 17 of 51 images\n",
      "downloading 18 of 51 images\n",
      "downloading 19 of 51 images\n",
      "downloading 20 of 51 images\n",
      "downloading 21 of 51 images\n",
      "downloading 22 of 51 images\n",
      "downloading 23 of 51 images\n",
      "downloading 24 of 51 images\n",
      "downloading 25 of 51 images\n",
      "downloading 26 of 51 images\n",
      "downloading 27 of 51 images\n",
      "downloading 28 of 51 images\n",
      "downloading 29 of 51 images\n",
      "downloading 30 of 51 images\n",
      "downloading 31 of 51 images\n",
      "downloading 32 of 51 images\n",
      "downloading 33 of 51 images\n",
      "downloading 34 of 51 images\n",
      "downloading 35 of 51 images\n",
      "downloading 36 of 51 images\n",
      "downloading 37 of 51 images\n",
      "downloading 38 of 51 images\n",
      "downloading 39 of 51 images\n",
      "downloading 40 of 51 images\n",
      "downloading 41 of 51 images\n",
      "downloading 42 of 51 images\n",
      "downloading 43 of 51 images\n",
      "downloading 44 of 51 images\n",
      "downloading 45 of 51 images\n",
      "downloading 46 of 51 images\n",
      "downloading 47 of 51 images\n",
      "downloading 48 of 51 images\n",
      "downloading 49 of 51 images\n",
      "downloading 50 of 51 images\n",
      "downloading 51 of 51 images\n"
     ]
    }
   ],
   "source": [
    "driver3=webdriver.Chrome(executable_path=driver_path)\n",
    "women_sarees=driver3.get(\"https://www.amazon.in/b/ref=s9_acss_bw_cg_WWDTIMPC_1a1_w?node=17186592031&pf_rd_m=A1K21FY43GMZF8&pf_rd_s=merchandised-search-1&pf_rd_r=ZH3S1Q55G1N7GKC76C78&pf_rd_t=101&pf_rd_p=279a5319-46a6-4963-b326-1fbf6cf3f419&pf_rd_i=1968256031\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-access-image cfMarker']\")\n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data\n",
    "\n",
    "\n",
    "product_details=scrap_images(driver=driver3)\n",
    "save_images(data=product_details,dirname=DIRNAME,page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 56 images\n",
      "downloading 2 of 56 images\n",
      "downloading 3 of 56 images\n",
      "downloading 4 of 56 images\n",
      "downloading 5 of 56 images\n",
      "downloading 6 of 56 images\n",
      "downloading 7 of 56 images\n",
      "downloading 8 of 56 images\n",
      "downloading 9 of 56 images\n",
      "downloading 10 of 56 images\n",
      "downloading 11 of 56 images\n",
      "downloading 12 of 56 images\n",
      "downloading 13 of 56 images\n",
      "downloading 14 of 56 images\n",
      "downloading 15 of 56 images\n",
      "downloading 16 of 56 images\n",
      "downloading 17 of 56 images\n",
      "downloading 18 of 56 images\n",
      "downloading 19 of 56 images\n",
      "downloading 20 of 56 images\n",
      "downloading 21 of 56 images\n",
      "downloading 22 of 56 images\n",
      "downloading 23 of 56 images\n",
      "downloading 24 of 56 images\n",
      "downloading 25 of 56 images\n",
      "downloading 26 of 56 images\n",
      "downloading 27 of 56 images\n",
      "downloading 28 of 56 images\n",
      "downloading 29 of 56 images\n",
      "downloading 30 of 56 images\n",
      "downloading 31 of 56 images\n",
      "downloading 32 of 56 images\n",
      "downloading 33 of 56 images\n",
      "downloading 34 of 56 images\n",
      "downloading 35 of 56 images\n",
      "downloading 36 of 56 images\n",
      "downloading 37 of 56 images\n",
      "downloading 38 of 56 images\n",
      "downloading 39 of 56 images\n",
      "downloading 40 of 56 images\n",
      "downloading 41 of 56 images\n",
      "downloading 42 of 56 images\n",
      "downloading 43 of 56 images\n",
      "downloading 44 of 56 images\n",
      "downloading 45 of 56 images\n",
      "downloading 46 of 56 images\n",
      "downloading 47 of 56 images\n",
      "downloading 48 of 56 images\n",
      "downloading 49 of 56 images\n",
      "downloading 50 of 56 images\n",
      "downloading 51 of 56 images\n",
      "downloading 52 of 56 images\n",
      "downloading 53 of 56 images\n",
      "downloading 54 of 56 images\n",
      "downloading 55 of 56 images\n",
      "downloading 56 of 56 images\n"
     ]
    }
   ],
   "source": [
    "driver3=webdriver.Chrome(executable_path=driver_path)\n",
    "women_sarees=driver3.get(\"https://www.amazon.in/s?rh=n%3A17186592031&page=2&qid=1593616856&ref=lp_17186592031_pg_2\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data\n",
    "\n",
    "\n",
    "product_details=scrap_images(driver=driver3)\n",
    "save_images(data=product_details,dirname=DIRNAME,page=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver3=webdriver.Chrome(executable_path=driver_path)\n",
    "women_sarees=driver3.get(\"https://www.amazon.in/s?rh=n%3A17186592031&page=3&qid=1593617649&ref=lp_17186592031_pg_3\")\n",
    "def scrap_images(driver):\n",
    "    images=driver.find_elements_by_xpath(\"//img[@class='s-image']\")\n",
    "    product_data= {}\n",
    "    product_data['images']=[]\n",
    "    for image in images:\n",
    "        source=image.get_attribute('src')\n",
    "        product_data['images'].append(source)\n",
    "        \n",
    "    return product_data\n",
    "\n",
    "\n",
    "product_details=scrap_images(driver=driver3)\n",
    "save_images(data=product_details,dirname=DIRNAME,page=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/prince/Desktop/x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUALLY RESIZING THE IMAGE SIZES FOR THE OUTPUT\n",
    "i_wd=150\n",
    "i_ht=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING CNN MODEL LAYERS\n",
    "model = Sequential()\n",
    "\n",
    "#FIRST CONV+RELU LAYER\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(i_wd,i_ht,3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(i_wd,i_ht,3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "#SECOND CONV+RELU LAYER\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(i_wd,i_ht,3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "#THIRD CONV+RELU LAYER\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(i_wd,i_ht,3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "#FULLY CONNECTED LAYER\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPILING THE MODEL\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"SGD\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA AUGMENTATION\n",
    "train_data_gen=ImageDataGenerator(rescale=1./255,horizontal_flip=True,zoom_range=0.2)\n",
    "valid_data_gen=ImageDataGenerator(rescale=1./255,horizontal_flip=True,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/princee/amazon data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = dataset.read_train_sets(path,(150,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 371 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen=train_data_gen.flow_from_directory(directory =\"C:/Users/prince/Desktop/x\",target_size=(150, 150),batch_size=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MEN': 0, 'WOMEN': 1, 'men2': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prince\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:2817: UserWarning: image file could not be identified because WEBP support not installed\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x000002531B9AB8E8>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-272-ee80f1ed5521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#FITTING , GENERATING ACCURACY AND LOSS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m     \u001b[0massert_not_namedtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    910\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[0;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                            interpolation=self.interpolation)\n\u001b[0m\u001b[0;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2816\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2817\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2818\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot identify image file %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x000002531B9AB8E8>"
     ]
    }
   ],
   "source": [
    "#FITTING , GENERATING ACCURACY AND LOSS\n",
    "history=model.fit_generator(train_gen,epochs = 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
